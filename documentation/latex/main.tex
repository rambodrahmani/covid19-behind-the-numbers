%-------------------------------------------------------------------------------
% File: main.tex
%	COVID-19 Behind the Numbers project documentation.
%
%	Compile using:
%	    $ pdflatex main.tex
%	    $ biber main
%
% Author: Rambod Rahmani <rambodrahmani@autistici.org>
%	  Created on 07/01/2021
%-------------------------------------------------------------------------------
\documentclass[11pt,a4paper]{article}

\usepackage[a4paper, portrait, margin=1.1in]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[linktoc=none]{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue,
}
\usepackage{listings}
\usepackage{float}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{bold-extra}

% time series countries colors
\definecolor{ts_belgium}{rgb}{0.11, 0.46, 0.70}
\definecolor{ts_armenia}{rgb}{0.99, 0.49, 0.5}
\definecolor{ts_austria}{rgb}{0.16, 0.62, 0.16}
\definecolor{ts_bulgaria}{rgb}{0.83, 0.14, 0.15}
\definecolor{ts_france}{rgb}{0.57, 0.40, 0.73}
\definecolor{ts_unitedstates}{rgb}{0.54, 0.33, 0.29}
\definecolor{ts_spain}{rgb}{0.88, 0.46, 0.75}
\definecolor{ts_germany}{rgb}{0.49, 0.49, 0.49}
\definecolor{ts_italy}{rgb}{0.73, 0.73, 0.12}
\definecolor{ts_brazil}{rgb}{0.0, 0.100, 0.84}

% bibliography references
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{main.bib}
\nocite{*}

\begin{document}

%-------------------------------------------------------------------------------
% Title
%-------------------------------------------------------------------------------
\begin{center}
	\huge{\bfseries{\scshape{COVID-19: Behind The Numbers}}}\\
	\vspace{1.0cm}
	\large{Data Mining and Machine Learning Project}\\
	\vspace{0.2cm}
	\large{Prof. Marcelloni Francesco}\\
	\vspace{0.2cm}
	\large{Prof. Ducange Pietro}\\
	\vspace{1.0cm}
	\large\textit{Rambod Rahmani}\\
	\vspace{0.2cm}
	\scriptsize{Master's Degree in Artificial Intelligence and
	Data Engineering}\\
	\vspace{1.0cm}
	\normalsize{\today}
\end{center}

%-------------------------------------------------------------------------------
% Table of contents
%-------------------------------------------------------------------------------
\vspace{2.0cm}
\tableofcontents

%-------------------------------------------------------------------------------
% Section: Introduction
%-------------------------------------------------------------------------------
\newpage
\section{Introduction}
\textbf{Decemebr 31, 2019}: \textit{Wuhan Municipal Health Commission, China,
reported a cluster of cases of pneumonia in Wuhan, Hubei Province.}\\
\\
\textbf{January 1, 2020}: \textit{World Health Organization (WHO) had set up the
Incident Management Support Team across the three levels of the organization.}\\
\\
\textbf{January 5, 2020}: \textit{WHO published the first Disease Outbreak News
on the new virus. This was a flagship technical publication to the scientific
community.}\\
\\
\textbf{January 12, 2020}: \textit{China publicly shared the genetic sequence of
COVID-19.}\\
\\
At the beginning of 2020, a new virus started spreading around in the capital of
Central China's Hubei province: the city we all came to know as Wuhan. As it
turned out, this was the start of a world-changing event with overwhelming
extent: Coronavirus Disease 2019 (COVID-19). After the first wave of the virus
has passed over the entire world, the aim of this work is to address the
following questions:
\begin{itemize}
	\item Which countries have been affected the most by COVID-19?
	\item Is it possible to build personalized predictive models for
		symptomatic COVID-19 patients based on basic health preconditions?
\end{itemize}
In order to fully answer these questions, first of all a reliable and big enough
dataset was needed. Second, Data Mining and Machine Learning techniques were
applied in order to obtain statistically significant results that could help
address the proposed questions. In the following pages the described work and
the resulting Python software is presented. The software architecture is
presented in the very last section in order to focus primarily on the dataset
retrieval and preprocessing, and on the analysis techniques and results.\\
\\
All the content related to this work can be found at:
\url{https://github.com/rambodrahmani/covid19-behind-the-numbers}

%-------------------------------------------------------------------------------
% Section: Dataset
%-------------------------------------------------------------------------------
\newpage
\section{Dataset}
Two different datasets were used:
\begin{itemize}
    \item the data on confirmed cases and confirmed deaths is updated daily and
    is published by \textbf{Johns Hopkins University}, the best available global
    dataset on the pandemic;
    \item as far as it concerns medical preconditions of COVID-19 patients,
    the most detailed dataset I was able to retrieve is the one provided by
    \textbf{The Federal government of Mexico}.
\end{itemize}
The content of the datasets, the feature of interest used in what follows and
the preprocessing procedures applied on each of them are detailed in the
following subsections.
\subsection{COVID-19 Daily Data}
The Johns Hopkins University Center for Systems Science and Engineering
(JHU CSSE)\footnote{\url{https://coronavirus.jhu.edu/map.html}} provides the
best available global dataset on the COVID-19 pandemic. Multiple sources were
used in the data set, since January 21, 2020:
\begin{itemize}
    \item World Health Organization (WHO);
    \item European Centre for Disease Prevention and Control (ECDC);
    \item US Centers for Disease Control and Prevention (UCDC);
    \item Los Angeles Times;
    \item The Mercury News;
\end{itemize}
The JHU CSSE data is provided as a collection of daily \texttt{.csv} files which
need to merged to obtain the dataset with all the daily data for all the
countries. Luckily, the \textbf{Our World in Data} organization - a
collaborative effort between researchers at the University of Oxford, focused on
"research and data to make progress against the world's largest problems" -
provides the JHU CSSE data already
merged\footnote{\url{https://ourworldindata.org/coronavirus-data}} and updated
to the latest second as a single \texttt{.csv} file. The choice was made to use
this \texttt{.csv} file as dataset. The Python program developed as part of this
work, provides the \texttt{update\_historical\_data} command which can be used
to fetch the latest available data automatically.\\
The dataset file is \texttt{owid-covid-data.csv} with a size of $16.5$ MiB.\\
\\
This dataset was used for the first part of the work: finding the characteristic
curves of each country and grouping together countries with comparable behavior
by means of clustering algorithms. This will provide us a truthful overview of
the countries most affected by COVID-19 and the ones that put in place
appropriate policies to handle the pandemic widespread.
\subsubsection{Preprocessing}
This dataset contains worldwide per-country daily data for COVID-19 for a total
of 59 columns and 68331 entries. Among others, we are interested mainly in
\begin{itemize}
    \item total confirmed cases;
    \item new confirmed cases;
    \item total deaths;
    \item new deaths;
    \item total confirmed cases per one million population;
    \item new confirmed cases per one million population;
    \item total deaths per one million population;
    \item new deaths per one million population;
\end{itemize}
The main considerations to be made about this dataset as far as it concerns data
preprocessing are:
\begin{itemize}
    \item it contains redundant data, e.g. values aggregated by continent (Asia,
    Africa, Europe, America, North America etc\dots);
    \item some of the daily data values are missing (specially for the very
    initial days of the pandemic, i.e. from 2020-01-01 to 2020-03-01)
\end{itemize}
as a result, the following preprocessing procedures were applied
\begin{itemize}
    \item data aggregated by continent was removed;
    \item missing daily data values for countries were replaced\footnote{The
    \texttt{scikit-learn SimpleImputer} was used to this end} with the constant
    value $0$; this seems the most reasonable choice since these missing values
    refer to the very beginning of the pandemic; using either the mean or the
    mode produces a distorted dataset.
    \item negative values were replaced with the constant value $0$ too; another
    strategy was tried first: using the moving average with window size $k$ to
    approximate the negative value as the average of the $k$ previous and
    successive values; however the problem was determining $k$.
\end{itemize}
Taking into account also the fact that the daily data will mostly be used
resampled with weekly frequency, there is not need to further preprocess the
dataset.
\subsection{COVID-19 Medical Preconditions Data}
The second part of this work focused on frequent pattern analysis in order to be
able to find interesting pattern as far as it concerns COVID-19 patients who
had prior medical preconditions. Finding such patterns is a crucial task that
might allow to best allocate very limited medical resources. The dataset
required for such type of analysis was not easy to find: usually COVID-19
datasets only contain daily values (numbers) of confirmed cases and deaths. They
rarely come equipped with the medical preconditions of the patients. Anyway,
the Federal government of Mexico\cite{mexico} provided such a dataset: it is
splitted in three main files:
\begin{itemize}
    \item \texttt{210206COVID19MEXICO.csv}: the main data file containing, among
    others,
    \begin{itemize}
        \item patients sex, age, admission date, COVID-19 test result;
        \item if the patient required hospitalization, intubation or Intensive
        care unit (ICU);
        \item if the patient was affected by Pneumonia, Diabetes, Chronic
        obstructive pulmonary disease (COPD), Asthma, Hypertension,
        Cardiovascular disease (CVD), etc\dots;
        \item death date (only for those patients who actually died).
    \end{itemize}
    \item \texttt{201128\_Catalogos.xlsx}, \texttt{201128\_Descriptores.xlsx}:
    contain additional clarifications regarding each feature present in the main
    dataset file.
\end{itemize}
The rapid global spread of the virus SARS-CoV-2 has provoked a spike in
demand for hospital care. Hospital systems across the world have been
over-extended, including the one case we are most familiar with, Northern Italy.
As a result, decisions on how to best allocate very limited medical resources
have come to the forefront: who to test, who to admit into hospitals, who to
treat in an Intensive Care Unit (ICU), who to support with a ventilator.
This dataset is perfect for the objective of the analysis we are interested in:
develop personalized models that predict the following events:
\begin{enumerate}
    \item hospitalization;
    \item need for ICU;
    \item need for a ventilator;
    \item mortality;
\end{enumerate}
\subsubsection{Preprocessing}
The preprocessing required by this second dataset is completely different from
the one required  by the first one. This is because the type of data is
completely different. While the first dataset is primarily a sequence of
numerical records, here we deal with categorical attributes. The main
considerations to be made include
\begin{itemize}
    \item the dataset is split into $3$ files: one with the main content, the
    remaining two files contain headers details;
    \item Spanish is the de facto national language spoken by the vast majority
    of Mexicans, all files are in Spanish;
    \item the dataset has a size of about $754$ MiB, with a total of 40 columns
    and 4.880.032 entries.
\end{itemize}
The preprocessing stage included merging the 3 files into a single \texttt{.csv}
file, in English, containing therefore patients details, medical preconditions,
required medical care and if they survived SARS-CoV-2 or not.

%-------------------------------------------------------------------------------
% Section: Analysis
%-------------------------------------------------------------------------------
\newpage
\section{Analysis}
As said in the introductory section, the analysis was carried out using data
mining and machine learning techniques in order to answer the proposed
questions. Each of the following subsections is focused on one of them.
\subsection{Which countries have been affected the most by COVID-19?}
To answer the very first question, we need to understand what is hidden behind
the official numbers and charts of confirmed COVID-19 active cases and deaths.
We are so used to watching them and sometimes we think we might even understand
how the COVID-19 pandemic is evolving as days goes by. For example, it is very
common to consider the following:
\begin{figure}[H]
    \begin{center}
        \hspace*{-0.2cm}
        \includegraphics[scale=0.32]{img/total-cases.pdf}
    \end{center}
    \caption{Top 15 Countries Confirmed COVID-19 Cases}
\end{figure}
\noindent Is this really the right choice? Does this ranking tells us anything
meaningful about the current undergoing pandemic situation? From what we can
observe in figure 1, clearly United States has higher confirmed COVID-19 cases
than countries such as Spain or Italy. Taking into account that the US is a much
bigger country, we can agree that this results do not imply that the US is more
affected than Spain, Italy or Germany. We can therefore think of a fairer
comparison independent of the country size: the number of infections needs to be
normalized to the population of each country. This provides a more coherent view
of the countries most affected by COVID-19:
\begin{figure}[H]
    \begin{center}
        \hspace*{-0.2cm}
        \includegraphics[scale=0.32]{img/total-cases-per-million.pdf}
    \end{center}
    \caption{Top 15 Countries Confirmed COVID-19 Cases Per One Million Population}
\end{figure}
\noindent However it is not yet good enough: countries have different testing
policies and a more intensive COVID-19 testing rate gives more confirmed cases
while no testing at all would imply zero cases. We can all agree upon the fact
that zero cases with no testing at all does not really mean that a given country
is not affected by the pandemic. We therefore need a quantity unrelated to the
rate of testing.\\
This quantity is the number of deaths: this value is unbiased by the testing
rate. We will use the normalized number of daily deaths for comparing
countries.\\
\\
While the previous results were obtained using the original COVID-19 historical
dataset\cite{ourworldindata}, \textbf{this time some preprocessing and
resampling is needed}:
\begin{itemize}
    \item the daily values for some of the dates in the original dataset are missing:
    \begin{itemize}
        \item countries with too many missing values (more than $150$ days) were
        removed since no interpolation technique can really be effective in such
        cases;
        \item were few missing values were present, these were replaced using
        the constant value $0$;
    \end{itemize}
    \item plotting the data with daily granularity results in a time series plot
    which is not smooth and hard to read; the data was therefore resampled with
    weekly granularity;
\end{itemize}
To impute the missing values, i.e., to infer them from the known part of the
data, an univariate imputation algorithm was used: imputes values in the
i-th feature dimension using only non-missing values in that feature dimension.
To this end, the \texttt{SimpleImputer} class from the \texttt{scikit-learn}
Python package was used.\\
\\
This is the resulting time series plot for some of the countries:
\begin{figure}[H]
    \begin{center}
        \hspace*{-0.3cm}
        \includegraphics[scale=0.325]{img/weekly-deaths-per-million.pdf}
    \end{center}
    \caption{COVID-19 Weekly Deaths Per One Million Population}
\end{figure}
\noindent Daily deaths count appears to be a fair measure to compare how hard
different countries have been hit by the virus. In addition, observing the plot
one can immediately point out that
\begin{itemize}
    \item some countries suffered more the first wave, some others the second
    and some both;
    \item countries such as {\color{ts_italy}Italy} and
    {\color{ts_belgium}Belgium} were devastated by the first wave and reacted
    by completely shutting down social life; yet they are not doing better with
    the second wave;
    \item countries such as {\color{ts_austria}Austria} and
    {\color{ts_germany}Germany} have not been hit hard by the first COVID-19
    wave; these states reacted fast and slowed down social life right at the
    beginning; as a result, the number of deaths went back to almost zero; yet
    they are suffering from the second wave;
    \item countries such as {\color{ts_unitedstates}United States} and
    {\color{cyan}Brazil} have a daily death toll with an almost constant
    trend.
\end{itemize}
If we extend the same reasoning to all the countries, figure 4 shows the weekly
deaths per one million population for 200 countries, from Afghanistan to
Zimbabwe. It is humanly impossible to extract any useful information from such a
plot. However, in this mess, how many different characteristic curves can we
find? To clean the mess, find patterns and extract the required knowledge, a
clustering algorithm was used. This unsupervised learning technique groups
similar data curves together.
\begin{figure}[H]
    \begin{center}
        \hspace*{-0.3cm}
        \includegraphics[scale=0.32]{img/weekly-deaths-worldwide.pdf}
    \end{center}
    \caption{COVID-19 Weekly Deaths Per One Million Population Worldwide}
\end{figure}
\subsubsection{Clustering time series data}
When taking into account time series, there have been several measures applied
(Aghabozorgi, Shirkhorshidi, and Wah (2015). p. 22). For example there is
probability-based distance, that takes into account the seasonality of data,
Hausdorff distance defined as "the maximum of the distances from a point in any
of the sets to the nearest point in the other set" (Rote (1991)) or Hidden
Markov model based distance used in modelling complex time series (Zeng, Duan,
and Wu (2010)). The most popular however is the Euclidean distance and Dynamic
Time Warping distance. It has been proven that the Euclidean distance is the
most efficient, but forces both time series to be the same length (Guijo-Rubio
et al. (2018); Wang et al. (2013)). DTW method is however known to be the most
accurate (Wang et al. (2013)).\\
\\
\textbf{Euclidean vs. Dynamic Time Warping}\\
The main difference between both distances can be best understood graphically.
The picture below shows an example of matched points of two data vectors. They
were connected based on the minimal distance between points based on DTW (black)
and Euclidean (red) distance. It can be seen that with DTW the 11th blue point
matches 4 green points. When taking into account the Euclidean distance it can
be seen that the assigned points 9-to-9 and 10-to-10 are visibly further than
9-to-11 and 10-to-11. That can significantly impact the overall distance of
series between each other. The DTW takes into account the shape of both time
series much better. Dynamic Time warping is a method of calculating distance
that is more accurate than Euclidean distance. It has an advantage over
Euclidean if data points are shifted between each other and we want to look
rather at its shape.
\begin{figure}[H]
    \begin{center}
        \hspace*{-0.3cm}
        \includegraphics[scale=1.1]{img/euclidean-dtw.png}
    \end{center}
    \caption{Visual comparison of matched points based on DTW (black) and Euclidean (red) distance}
\end{figure}
\noindent Additionally two time series don't have to be equal in length what is
an assumption required by the Euclidean distance. The Euclidean distance takes
pairs of data points and compares them to each other. DTW calculates the
smallest distance between all points - this enables a one-to-many match.\\
\\
Two clustering models were built: one using the euclidean distance and one using
the DTW algorithm. To this end, \texttt{tslearn}, a Python package that provides
machine learning tools for the analysis of time series, was used. This package
builds on (and hence depends on) \texttt{scikit-learn}, \texttt{numpy} and
\texttt{scipy} libraries.
\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.32]{img/daily-deaths-euclidean-clusters.pdf}
    \end{center}
    \caption{Time series K-means Clustering Model using Euclidean distance}
\end{figure}
\noindent figure 6 shows the clusters obtained by applying the K-means
Clustering algorithm provided by the \texttt{TimeSeriesKMeans} class. In this
model, the metric chosen to be used for both cluster assignment and barycenter
computation is the euclidean distance.
The {\color{ForestGreen}green cluster} groups $138$ countries with daily deaths
close to zero. The {\color{red}red cluster} groups $26$ countries with a huge
number of daily deaths, that managed to get the situation controlled for a
while, but are now suffering for the second wave. The {\color{blue}blue cluster}
groups $36$ countries which suffer constantly COVID-19 deaths and where it seems
the situation is getting worse and worse over time.
\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.32]{img/daily-deaths-dtw-clusters.pdf}
    \end{center}
    \caption{Time series K-means Clustering Model using Dynamic Time Warping algorithm}
\end{figure}
\noindent The time series K-means clustering model using the Dynamic Time
Warping algorithm provides different clusters. This time,
\begin{itemize}
    \item the {\color{ForestGreen}green cluster} groups $139$ countries;
    \item the {\color{red}green cluster} groups $16$ countries;
    \item the {\color{blue}green cluster} groups $45$ countries.
\end{itemize}
We should keep in mind that the DTW algorithm is quite CPU-intensive: as a
matter of fact, with only $10$ iterations, it takes almost $1$ minute to
compute the clusters shown in figure 7.\\
To summarize, by means of a machine learning algorithm applied to the COVID-19
data we organized countries into groups with similar epidemiological behavior.
Surprisingly, those groups form local clusters on the world map as well.
\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.9]{img/clusters-map.png}
    \end{center}
    \caption{K-means Clustering Model Map}
\end{figure}
\subsection{Personalized predictive models for symptomatic COVID-19 patients}

%-------------------------------------------------------------------------------
% Section: Conclusion
%-------------------------------------------------------------------------------
\section{Conclusion}

%-------------------------------------------------------------------------------
% Section: Software Architecture
%-------------------------------------------------------------------------------
\section{Software Architecture}

\printbibliography

\end{document}
